{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "import torch\n",
    "from torchvision.transforms.functional import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.archs import * #register the arch\n",
    "from src.utils import imwrite, img2tensor, tensor2img\n",
    "from src.utils.download_util import load_file_from_url\n",
    "from src.utils.misc import get_device\n",
    "from src.utils.registry import ARCH_REGISTRY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = get_device()\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-i', '--input_path', type=str, default='./inputs/masked_faces', \n",
    "                help='Input image or folder. Default: inputs/masked_faces')\n",
    "parser.add_argument('-o', '--output_path', type=str, default='results', \n",
    "                help='Output folder. Default: results/<input_name>')\n",
    "parser.add_argument('--suffix', type=str, default='_0514', \n",
    "                help='Suffix of the restored faces. Default: None')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] The input face images should be aligned and cropped to a resolution of 512x512.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ------------------------ input & output ------------------------\n",
    "print('[NOTE] The input face images should be aligned and cropped to a resolution of 512x512.')\n",
    "if args.input_path.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')): # input single img path\n",
    "    input_img_list = [args.input_path]\n",
    "    result_root = f'results/test_inpainting_img'\n",
    "else: # input img folder\n",
    "    if args.input_path.endswith('/'):  # solve when path ends with /\n",
    "        args.input_path = args.input_path[:-1]\n",
    "    # scan all the jpg and png images\n",
    "    input_img_list = sorted(glob.glob(os.path.join(args.input_path, '*.[jpJP][pnPN]*[gG]')))\n",
    "    result_root = f'results/{os.path.basename(args.input_path)}'\n",
    "\n",
    "if not args.output_path is None: # set output path\n",
    "    result_root = args.output_path\n",
    "\n",
    "\n",
    "test_img_num = len(input_img_list)\n",
    "test_img_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeFormer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------ set up CodeFormer restorer -------------------\n",
    "net = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=512, n_head=8, n_layers=9, \n",
    "                                        connect_list=['32', '64', '128']).to(device)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_path = 'weights/codeformer/codeformer_inpainting.pth'\n",
    "# /root/ref/weights/CodeFormer/inpainting.pth\n",
    "# ckpt_path = load_file_from_url(url=pretrain_model_url, \n",
    "#                                 model_dir='weights/CodeFormer', progress=True, file_name=None)\n",
    "checkpoint = torch.load(ckpt_path)['params_ema']\n",
    "net.load_state_dict(checkpoint)\n",
    "net.eval()\n",
    "print(net.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/13] Processing: 00105.png\n",
      "[2/13] Processing: 00108.png\n",
      "[3/13] Processing: 00169.png\n",
      "[4/13] Processing: 00588.png\n",
      "[5/13] Processing: 00588_2.png\n",
      "[6/13] Processing: 00588_3.png\n",
      "[7/13] Processing: 00664.png\n",
      "[8/13] Processing: 00664__05142.png\n",
      "[9/13] Processing: test25.png\n",
      "[10/13] Processing: test25_resize.png\n",
      "[11/13] Processing: test26.png\n",
      "[12/13] Processing: test27.png\n",
      "[13/13] Processing: test28.png\n",
      "\n",
      "All results are saved in results\n"
     ]
    }
   ],
   "source": [
    "# -------------------- start to processing ---------------------\n",
    "for i, img_path in enumerate(input_img_list):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    basename, ext = os.path.splitext(img_name)\n",
    "    print(f'[{i+1}/{test_img_num}] Processing: {img_name}')\n",
    "    input_face = cv2.imread(img_path)\n",
    "    assert input_face.shape[:2] == (512, 512), 'Input resolution must be 512x512 for inpainting.'\n",
    "    # input_face = cv2.resize(input_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "    input_face = img2tensor(input_face / 255., bgr2rgb=True, float32=True)\n",
    "    normalize(input_face, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "    input_face = input_face.unsqueeze(0).to(device)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            mask = torch.zeros(512, 512)\n",
    "            m_ind = torch.sum(input_face[0], dim=0)\n",
    "            mask[m_ind==3] = 1.0\n",
    "            mask = mask.view(1, 1, 512, 512).to(device)\n",
    "            # w is fixed to 1, adain=False for inpainting\n",
    "            output_face = net(input_face, w=1, adain=False)[0]\n",
    "            output_face = (1-mask)*input_face + mask*output_face\n",
    "            save_face = tensor2img(output_face, rgb2bgr=True, min_max=(-1, 1))\n",
    "        del output_face\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as error:\n",
    "        print(f'\\tFailed inference for CodeFormer: {error}')\n",
    "        save_face = tensor2img(input_face, rgb2bgr=True, min_max=(-1, 1))\n",
    "\n",
    "    save_face = save_face.astype('uint8')\n",
    "\n",
    "    # save face\n",
    "    if args.suffix is not None:\n",
    "        basename = f'{basename}_{args.suffix}'\n",
    "    save_restore_path = os.path.join(result_root, f'{basename}.png')\n",
    "    imwrite(save_face, save_restore_path)\n",
    "    # break\n",
    "\n",
    "print(f'\\nAll results are saved in {result_root}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
