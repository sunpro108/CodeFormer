{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "from src.utils import imwrite, img2tensor, tensor2img\n",
    "from src.utils.registry import ARCH_REGISTRY\n",
    "from src.archs import * # !register architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-i', '--test_path', type=str, default='data/iharmony4/HAdobe5k/real_images')\n",
    "parser.add_argument('-o', '--save_root', type=str, default='./experiments/pretrained_models/vqgan')\n",
    "parser.add_argument('--codebook_size', type=int, default=512)\n",
    "parser.add_argument('--ckpt_path', type=str, default='experiments/20240731_115629_VQGAN-512-ds32-nearest-stage1/models/net_g_80000.pth')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.save_root.endswith('/'):  # solve when path ends with /\n",
    "    args.save_root = args.save_root[:-1]\n",
    "dir_name = os.path.abspath(args.save_root)\n",
    "os.makedirs(dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_path = args.test_path\n",
    "save_root = args.save_root\n",
    "ckpt_path = args.ckpt_path\n",
    "codebook_size = args.codebook_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqgan = ARCH_REGISTRY.get('VQAutoEncoder')(512, 64, [1, 2, 2, 4, 4, 8], 'nearest',\n",
    "                                            codebook_size=codebook_size).to(device)\n",
    "checkpoint = torch.load(ckpt_path)['params_ema']\n",
    "\n",
    "vqgan.load_state_dict(checkpoint)\n",
    "vqgan.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_latent = np.zeros((codebook_size)).astype('float64')\n",
    "size_latent = 16\n",
    "latent = {}\n",
    "latent['orig'] = {}\n",
    "latent['hflip'] = {}\n",
    "for i in ['orig', 'hflip']:\n",
    "# for i in ['hflip']:\n",
    "    for img_path in sorted(glob.glob(os.path.join(test_path, '*.[jp][pn]g'))):\n",
    "        img_name = os.path.basename(img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        if i == 'hflip':\n",
    "            cv2.flip(img, 1, img)\n",
    "        img = img2tensor(img / 255., bgr2rgb=True, float32=True)\n",
    "        normalize(img, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            # output = net(img)[0]\n",
    "            # x, feat_dict = vqgan.encoder(img)\n",
    "            # x, _, log = vqgan.quantize(x)\n",
    "            x, codebook_loss, quant_stats = vqgan(img)\n",
    "        # del output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        min_encoding_indices = log['min_encoding_indices']\n",
    "        min_encoding_indices = min_encoding_indices.view(size_latent,size_latent)\n",
    "        latent[i][img_name[:-4]] = min_encoding_indices.cpu().numpy()\n",
    "        print(img_name, latent[i][img_name[:-4]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_save_path = os.path.join(save_root, f'latent_gt_code{codebook_size}.pth')\n",
    "torch.save(latent, latent_save_path)\n",
    "print(f'\\nLatent GT code are saved in {save_root}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
